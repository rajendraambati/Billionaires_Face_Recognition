{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dq9eHlwq2wMZ",
        "outputId": "92ebe1be-ff75-4afa-9d66-2f4aaeda04d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Dec  9 10:23:44 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdFoS1QT22lH",
        "outputId": "065c172a-2aa6-4d07-e71c-ef206bc69131"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: facenet-pytorch in /usr/local/lib/python3.10/dist-packages (2.6.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (1.26.4)\n",
            "Requirement already satisfied: Pillow<10.3.0,>=10.2.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (10.2.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (2.32.3)\n",
            "Requirement already satisfied: torch<2.3.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (2.2.2)\n",
            "Requirement already satisfied: torchvision<0.18.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (0.17.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (4.66.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3.0,>=2.2.0->facenet-pytorch) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet-pytorch) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.3.0,>=2.2.0->facenet-pytorch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install facenet-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "6c7bd4e3f9924585949af335263efb6f",
            "c9e16ca19830456491a4fc899243af64",
            "53ea70dd2a7247f9a52a478ac14c8649",
            "afbaad0b5c2b4150bd65def29a8b31c3",
            "22b88c5e29c54798986c9ff2f7e758e6",
            "934f7887ceaf4a778fb369e8e145d255",
            "fecf6f3371354631a9be7ac5def7e756",
            "9cc649fa41cf43ad8d0fe0689402cc54",
            "541ea67d7ad5428c9da4cf9bbd9b6000",
            "3e0db44009e2456b9ad9e95acda284f2",
            "94e62efd0232489abdb5257868ea0c22"
          ]
        },
        "id": "3FIcmLbR8tCB",
        "outputId": "fd519375-c271-4160-db73-e807992b53be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/107M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c7bd4e3f9924585949af335263efb6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading image from elon.jpg\n",
            "Output saved at: output_image.jpg\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Initialize device\n",
        "device = torch.device('cpu')\n",
        "\n",
        "# Initialize MTCNN for face detection\n",
        "mtcnn = MTCNN(keep_all=True, device=device)\n",
        "\n",
        "# Initialize Inception Resnet V1 for face recognition\n",
        "model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
        "\n",
        "def extract_face_embeddings(image, boxes):\n",
        "    \"\"\"Extract face embeddings from an image and bounding boxes.\"\"\"\n",
        "    faces = []\n",
        "    for box in boxes:\n",
        "        # Ensure bounding box coordinates are integers\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "\n",
        "        # Clamp coordinates to image boundaries\n",
        "        x1, y1 = max(0, x1), max(0, y1)\n",
        "        x2, y2 = min(image.shape[1], x2), min(image.shape[0], y2)\n",
        "\n",
        "        # Crop the face\n",
        "        face = image[y1:y2, x1:x2]\n",
        "\n",
        "        # Skip if the face crop is empty\n",
        "        if face.size == 0:\n",
        "            continue\n",
        "\n",
        "        # Resize the face to 160x160 pixels\n",
        "        face = cv2.resize(face, (160, 160))\n",
        "\n",
        "        # Normalize and reorder dimensions for the model\n",
        "        face = np.transpose(face, (2, 0, 1)) / 255.0\n",
        "        face = torch.tensor(face, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "        faces.append(face)\n",
        "\n",
        "    # Return embeddings if any faces were found\n",
        "    if len(faces) > 0:\n",
        "        faces = torch.cat(faces)\n",
        "        embeddings = model(faces).detach().cpu().numpy()\n",
        "        return embeddings\n",
        "\n",
        "    return None\n",
        "\n",
        "def add_faces_from_folder(folder_path, database):\n",
        "    \"\"\"Add all faces from a folder to the database using folder names as labels.\"\"\"\n",
        "    for label in os.listdir(folder_path):\n",
        "        label_folder = os.path.join(folder_path, label)\n",
        "        if os.path.isdir(label_folder):\n",
        "            for image_name in os.listdir(label_folder):\n",
        "                image_path = os.path.join(label_folder, image_name)\n",
        "                if image_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    img = cv2.imread(image_path)\n",
        "                    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                    boxes, _ = mtcnn.detect(img_rgb)\n",
        "\n",
        "                    if boxes is not None:\n",
        "                        embeddings = extract_face_embeddings(img_rgb, boxes)\n",
        "                        if embeddings is not None:\n",
        "                            # Store all embeddings for each label (person)\n",
        "                            if label not in database:\n",
        "                                database[label] = []\n",
        "                            database[label].append(embeddings[0])\n",
        "\n",
        "def detect_faces(image_path, database):\n",
        "    \"\"\"Detect faces in an image and draw bounding boxes with labels.\"\"\"\n",
        "    print(f\"Reading image from {image_path}\")\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(\"Error: Unable to load image.\")\n",
        "        return\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    boxes, _ = mtcnn.detect(img_rgb)\n",
        "\n",
        "    if boxes is not None:\n",
        "        embeddings = extract_face_embeddings(img_rgb, boxes)\n",
        "\n",
        "        for box, face_embedding in zip(boxes, embeddings):\n",
        "            x1, y1, x2, y2 = map(int, box)\n",
        "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "            # Compare with known embeddings in the database\n",
        "            min_distance = float('inf')\n",
        "            second_min_distance = float('inf')\n",
        "            best_match = \"Unknown\"\n",
        "\n",
        "            for name, known_embeddings in database.items():\n",
        "                distances = [np.linalg.norm(face_embedding - known_emb) for known_emb in known_embeddings]\n",
        "                current_min_distance = min(distances)\n",
        "\n",
        "                if current_min_distance < min_distance:\n",
        "                    second_min_distance = min_distance\n",
        "                    min_distance = current_min_distance\n",
        "                    best_match = name\n",
        "                elif current_min_distance < second_min_distance:\n",
        "                    second_min_distance = current_min_distance\n",
        "\n",
        "            # Assign label only if there's a significant gap between the closest and second-closest match\n",
        "            if min_distance < 0.9 and (second_min_distance - min_distance) > 0.1:  # Gap of 0.1\n",
        "                label = best_match\n",
        "            else:\n",
        "                label = \"Unknown\"\n",
        "\n",
        "            # Put label above the bounding box\n",
        "            cv2.putText(img, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
        "\n",
        "        # Save the output image\n",
        "        output_image_path = 'output_image.jpg'\n",
        "        cv2.imwrite(output_image_path, img)\n",
        "        print(f\"Output saved at: {output_image_path}\")\n",
        "    else:\n",
        "        print(\"No faces detected.\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize an empty database for storing known faces.\n",
        "    face_database = {}\n",
        "\n",
        "    # Path to the main folder containing subfolders with images on Google Drive.\n",
        "    main_folder_path = '/content/drive/MyDrive/bill'  # Replace with your actual folder path\n",
        "\n",
        "    # Add known faces from folders to the database.\n",
        "    add_faces_from_folder(main_folder_path, face_database)\n",
        "\n",
        "    # Recognize an unknown face.\n",
        "    unknown_image_path = 'elon.jpg'  # Replace with your unknown image path\n",
        "\n",
        "    # Detect faces in the provided unknown image.\n",
        "    detect_faces(unknown_image_path, face_database)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save that face_database for deployment using streamlit\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Save the face_database to a file\n",
        "with open('face_database.pkl', 'wb') as f:\n",
        "    pickle.dump(face_database, f)\n",
        "\n",
        "print(\"Face database saved to face_database.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF6FJGGu9e5O",
        "outputId": "0ef3d028-77ef-469c-afc3-2a46844662c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Face database saved to face_database.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'inception_resnet_v1_weights.pth')"
      ],
      "metadata": {
        "id": "JLv_vvAMueyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(mtcnn.state_dict(), 'inception_resnet_v1_mtcnn.pth')"
      ],
      "metadata": {
        "id": "hOZ9gtlxtCNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: i have both 'inception_resnet_v1_mtcnn.pth' file and 'inception_resnet_v1_weights.pth' file, and i have face_database.pkl file also, i want to use these models and face_database.pkl  to deploy in streamlit\n",
        "\n",
        "import streamlit as st\n",
        "import os\n",
        "import torch\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pickle\n",
        "\n",
        "# Load the pre-trained models and face database\n",
        "device = torch.device('cpu')\n",
        "mtcnn = MTCNN(keep_all=True, device=device)\n",
        "model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
        "\n",
        "# Load model weights\n",
        "mtcnn.load_state_dict(torch.load('inception_resnet_v1_mtcnn.pth', map_location=device))\n",
        "model.load_state_dict(torch.load('inception_resnet_v1_weights.pth', map_location=device))\n",
        "\n",
        "\n",
        "with open('face_database.pkl', 'rb') as f:\n",
        "    face_database = pickle.load(f)\n",
        "\n",
        "# Function to extract embeddings (same as before)\n",
        "def extract_face_embeddings(image, boxes):\n",
        "    # ... (Your extract_face_embeddings function from the previous code)\n",
        "\n",
        "# Function to detect faces and display results in Streamlit\n",
        "def detect_faces_streamlit(image, database):\n",
        "    # ... (Your detect_faces function with slight modifications for Streamlit)\n",
        "    # Instead of saving output_image.jpg, you'll display img directly in Streamlit\n",
        "    # Example:\n",
        "    # st.image(img, caption='Processed Image', use_column_width=True)\n",
        "\n",
        "\n",
        "# Streamlit app\n",
        "st.title(\"Face Recognition App\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # Convert the file to an opencv image.\n",
        "    file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)\n",
        "    opencv_image = cv2.imdecode(file_bytes, 1)\n",
        "\n",
        "    # Now do something with the image! For example, let's display it:\n",
        "    st.image(opencv_image, channels=\"BGR\")\n",
        "\n",
        "    detect_faces_streamlit(opencv_image, face_database)"
      ],
      "metadata": {
        "id": "pbeO0-w0uvyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write full code not suggestions\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab import drive\n",
        "!nvidia-smi\n",
        "!pip install facenet-pytorch\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Initialize device\n",
        "device = torch.device('cpu')\n",
        "\n",
        "# Initialize MTCNN for face detection\n",
        "mtcnn = MTCNN(keep_all=True, device=device)\n",
        "\n",
        "# Initialize Inception Resnet V1 for face recognition\n",
        "model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
        "\n",
        "def extract_face_embeddings(image, boxes):\n",
        "    \"\"\"Extract face embeddings from an image and bounding boxes.\"\"\"\n",
        "    faces = []\n",
        "    for box in boxes:\n",
        "        # Ensure bounding box coordinates are integers\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "\n",
        "        # Clamp coordinates to image boundaries\n",
        "        x1, y1 = max(0, x1), max(0, y1)\n",
        "        x2, y2 = min(image.shape[1], x2), min(image.shape[0], y2)\n",
        "\n",
        "        # Crop the face\n",
        "        face = image[y1:y2, x1:x2]\n",
        "\n",
        "        # Skip if the face crop is empty\n",
        "        if face.size == 0:\n",
        "            continue\n",
        "\n",
        "        # Resize the face to 160x160 pixels\n",
        "        face = cv2.resize(face, (160, 160))\n",
        "\n",
        "        # Normalize and reorder dimensions for the model\n",
        "        face = np.transpose(face, (2, 0, 1)) / 255.0\n",
        "        face = torch.tensor(face, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "        faces.append(face)\n",
        "\n",
        "    # Return embeddings if any faces were found\n",
        "    if len(faces) > 0:\n",
        "        faces = torch.cat(faces)\n",
        "        embeddings = model(faces).detach().cpu().numpy()\n",
        "        return embeddings\n",
        "\n",
        "    return None\n",
        "\n",
        "def add_faces_from_folder(folder_path, database):\n",
        "    \"\"\"Add all faces from a folder to the database using folder names as labels.\"\"\"\n",
        "    for label in os.listdir(folder_path):\n",
        "        label_folder = os.path.join(folder_path, label)\n",
        "        if os.path.isdir(label_folder):\n",
        "            for image_name in os.listdir(label_folder):\n",
        "                image_path = os.path.join(label_folder, image_name)\n",
        "                if image_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    img = cv2.imread(image_path)\n",
        "                    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                    boxes, _ = mtcnn.detect(img_rgb)\n",
        "\n",
        "                    if boxes is not None:\n",
        "                        embeddings = extract_face_embeddings(img_rgb, boxes)\n",
        "                        if embeddings is not None:\n",
        "                            # Store all embeddings for each label (person)\n",
        "                            if label not in database:\n",
        "                                database[label] = []\n",
        "                            database[label].append(embeddings[0])\n",
        "\n",
        "def detect_faces(image_path, database):\n",
        "    \"\"\"Detect faces in an image and draw bounding boxes with labels.\"\"\"\n",
        "    print(f\"Reading image from {image_path}\")\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(\"Error: Unable to load image.\")\n",
        "        return\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    boxes, _ = mtcnn.detect(img_rgb)\n",
        "\n",
        "    if boxes is not None:\n",
        "        embeddings = extract_face_embeddings(img_rgb, boxes)\n",
        "\n",
        "        for box, face_embedding in zip(boxes, embeddings):\n",
        "            x1, y1, x2, y2 = map(int, box)\n",
        "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "            # Compare with known embeddings in the database\n",
        "            min_distance = float('inf')\n",
        "            second_min_distance = float('inf')\n",
        "            best_match = \"Unknown\"\n",
        "\n",
        "            for name, known_embeddings in database.items():\n",
        "                distances = [np.linalg.norm(face_embedding - known_emb) for known_emb in known_embeddings]\n",
        "                current_min_distance = min(distances)\n",
        "\n",
        "                if current_min_distance < min_distance:\n",
        "                    second_min_distance = min_distance\n",
        "                    min_distance = current_min_distance\n",
        "                    best_match = name\n",
        "                elif current_min_distance < second_min_distance:\n",
        "                    second_min_distance = current_min_distance\n",
        "\n",
        "            # Assign label only if there's a significant gap between the closest and second-closest match\n",
        "            if min_distance < 0.9 and (second_min_distance - min_distance) > 0.1:  # Gap of 0.1\n",
        "                label = best_match\n",
        "            else:\n",
        "                label = \"Unknown\"\n",
        "\n",
        "            # Put label above the bounding box\n",
        "            cv2.putText(img, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
        "\n",
        "        # Save the output image\n",
        "        output_image_path = 'output_image.jpg'\n",
        "        cv2.imwrite(output_image_path, img)\n",
        "        print(f\"Output saved at: {output_image_path}\")\n",
        "    else:\n",
        "        print(\"No faces detected.\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize an empty database for storing known faces.\n",
        "    face_database = {}\n",
        "\n",
        "    # Path to the main folder containing subfolders with images on Google Drive.\n",
        "    main_folder_path = '/content/drive/MyDrive/bill'  # Replace with your actual folder path\n",
        "\n",
        "    # Add known faces from folders to the database.\n",
        "    add_faces_from_folder(main_folder_path, face_database)\n",
        "\n",
        "    # Recognize an unknown face.\n",
        "    unknown_image_path = 'elon.jpg'  # Replace with your unknown image path\n",
        "\n",
        "    # Detect faces in the provided unknown image.\n",
        "    detect_faces(unknown_image_path, face_database)\n",
        "\n",
        "torch.save(model.state_dict(), 'inception_resnet_v1_weights.pth')\n",
        "torch.save(mtcnn.state_dict(), 'inception_resnet_v1_mtcnn.pth')\n",
        "\n",
        "\n",
        "# Initialize device\n",
        "device = torch.device('cpu')\n",
        "\n",
        "# Initialize MTCNN for face detection\n",
        "mtcnn = MTCNN(keep_all=True, device=device)\n",
        "\n",
        "# Initialize Inception Resnet V1 for face recognition\n",
        "model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
        "\n",
        "# Load pre-trained weights if available\n",
        "if os.path.exists('inception_resnet_v1_weights.pth'):\n",
        "    model.load_state_dict(torch.load('inception_resnet_v1_weights.pth', map_location=device))\n",
        "    print(\"Loaded InceptionResnetV1 weights\")\n",
        "else:\n",
        "    print(\"InceptionResnetV1 weights not found. Using default weights.\")\n",
        "\n",
        "if os.path.exists('inception_resnet_v1_mtcnn.pth'):\n",
        "    mtcnn.load_state_dict(torch.load('inception_resnet_v1_mtcnn.pth', map_location=device))\n",
        "    print(\"Loaded MTCNN weights\")\n",
        "else:\n",
        "    print(\"MTCNN weights not found. Using default weights.\")\n",
        "\n",
        "\n",
        "def extract_face_embeddings(image, boxes):\n",
        "    \"\"\"Extract face embeddings from an image and bounding boxes.\"\"\"\n",
        "    # ... (rest of the function remains the same)\n",
        "\n",
        "def add_faces_from_folder(folder_path, database):\n",
        "    \"\"\"Add all faces from a folder to the database using folder names as labels.\"\"\"\n",
        "    # ... (rest of the function remains the same)\n",
        "\n",
        "def detect_faces(image_path, database):\n",
        "    \"\"\"Detect faces in an image and draw bounding boxes with labels.\"\"\"\n",
        "    # ... (rest of the function remains the same)\n",
        "\n",
        "\n",
        "# Example usage (remove for Streamlit deployment)\n",
        "# if __name__ == \"__main__\":\n",
        "#     # ... (example usage code)"
      ],
      "metadata": {
        "id": "HNchJ8dF6Tjh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6c7bd4e3f9924585949af335263efb6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c9e16ca19830456491a4fc899243af64",
              "IPY_MODEL_53ea70dd2a7247f9a52a478ac14c8649",
              "IPY_MODEL_afbaad0b5c2b4150bd65def29a8b31c3"
            ],
            "layout": "IPY_MODEL_22b88c5e29c54798986c9ff2f7e758e6"
          }
        },
        "c9e16ca19830456491a4fc899243af64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_934f7887ceaf4a778fb369e8e145d255",
            "placeholder": "​",
            "style": "IPY_MODEL_fecf6f3371354631a9be7ac5def7e756",
            "value": "100%"
          }
        },
        "53ea70dd2a7247f9a52a478ac14c8649": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cc649fa41cf43ad8d0fe0689402cc54",
            "max": 111898327,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_541ea67d7ad5428c9da4cf9bbd9b6000",
            "value": 111898327
          }
        },
        "afbaad0b5c2b4150bd65def29a8b31c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e0db44009e2456b9ad9e95acda284f2",
            "placeholder": "​",
            "style": "IPY_MODEL_94e62efd0232489abdb5257868ea0c22",
            "value": " 107M/107M [00:00&lt;00:00, 169MB/s]"
          }
        },
        "22b88c5e29c54798986c9ff2f7e758e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "934f7887ceaf4a778fb369e8e145d255": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fecf6f3371354631a9be7ac5def7e756": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cc649fa41cf43ad8d0fe0689402cc54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "541ea67d7ad5428c9da4cf9bbd9b6000": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e0db44009e2456b9ad9e95acda284f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94e62efd0232489abdb5257868ea0c22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}